import sys
import tqdm
import numpy as np
import torch
import torch.utils.data as data_utils
from torch.utils.data.sampler import SubsetRandomSampler

from gurobipy import *
from types import SimpleNamespace

from facilityNN import FacilityNN, FeatureNN

# uncapacitated facility location problem
def generateInstance(n, m):
    # while True:
    #     G = nx.random_geometric_graph(n+m, p)
    #     if nx.is_connected(G):
    #         break
    # pos = nx.get_node_attributes(G, 'pos')
    # for e in G.edges():
    #     G[e]['weight'] = np.linalg.norm(np.array(pos[e[0]]) - np.array(pos[e[1]]))
    # c = distance[:n,n:] # shipping cost per product

    c = 5 * np.random.random((n,m))
    d = 10 * np.ones(m) # customer demand
    f = 10 * np.ones(n) # facility open cost
    # d = np.random.random(m) # customer demand
    # f = np.random.random(n) # facility open cost
    return SimpleNamespace(n=n, m=m, c=c, d=d, f=f)

def generateFeatures(n, m, instances, feature_size=32):
    labels = torch.Tensor([instance.c for instance in instances])
    feature_net = FeatureNN(input_shape=(n,m), output_shape=(n,feature_size))
    feature_net.eval()
    features = feature_net(labels).detach()
    return features, labels

def generateDataset(n, m, num_instances, feature_size=32):
    instances = [generateInstance(n,m) for i in range(num_instances)]
    features, labels = generateFeatures(n, m, instances, feature_size=feature_size)

    train_size    = int(np.floor(num_instances * 0.7))
    validate_size = int(np.floor(num_instances * 0.1)) 
    test_size     = num_instances - train_size - validate_size

    entire_dataset = data_utils.TensorDataset(features, labels)

    indices = list(range(num_instances))
    np.random.shuffle(indices)

    train_indices    = indices[:train_size]
    validate_indices = indices[train_size:train_size+validate_size]
    test_indices     = indices[train_size+validate_size:]

    train_loader    = data_utils.DataLoader(entire_dataset, batch_size=16, sampler=SubsetRandomSampler(train_indices))
    validate_loader = data_utils.DataLoader(entire_dataset, batch_size=16, sampler=SubsetRandomSampler(validate_indices))
    test_loader     = data_utils.DataLoader(entire_dataset, batch_size=16, sampler=SubsetRandomSampler(test_indices))

    return SimpleNamespace(train=train_loader, test=test_loader, validate=validate_loader)

def MILPSolver(instance):
    model = Model()
    model.params.OutputFlag=0
    model.params.TuneOutput=0
    n, m, c, d, f = instance.n, instance.m, instance.c, instance.d, instance.f

    # adding variables
    x = model.addVars(n, vtype=GRB.BINARY)
    z = model.addVars(n, m, vtype=GRB.BINARY)

    # adding constraints
    for j in range(m):
        model.addConstr(z.sum('*', j) == 1, name='customer{}'.format(j))
        for i in range(n):
            model.addConstr(z[i,j] <= x[i], name='supply{},{}'.format(i,j))

    obj1 = quicksum(c[i,j] * d[j] * z[i,j] for i in range(n) for j in range(m))
    obj2 = quicksum(f[i] * x[i] for i in range(n))
    obj = obj1 + obj2
    model.setObjective(obj, GRB.MINIMIZE)

    model.update()
    model.write("milp.lp")
    model.optimize()
    x_values = np.array([x[i].x for i in range(n)])
    z_values = np.array([[z[i,j].x for j in range(m)] for i in range(n)])
    obj_value = model.objVal

    print(x_values, z_values)
    print('optimal:', obj_value)
    return SimpleNamespace(x=x_values, z=z_values, obj=obj_value)

def LPSolver(instance):
    model = Model()
    model.params.OutputFlag=0
    model.params.TuneOutput=0
    n, m, c, d, f = instance.n, instance.m, instance.c, instance.d, instance.f

    # Adding variables
    x = model.addVars(n, lb=0.0, ub=1.0, vtype=GRB.CONTINUOUS)
    z = model.addVars(n, m, lb=0.0, ub=1.0, vtype=GRB.CONTINUOUS)

    # Adding constraints
    for j in range(m):
        model.addConstr(z.sum('*', j) == 1, name='customer{}'.format(j))

    # Better formulation
    for i in range(n):
        for j in range(m):
            model.addConstr(z[i,j] <= x[i], name='supply{},{}'.format(i,j))

    # Loosen formulation
    # M = 1000
    # for i in range(n):
    #     model.addConstr(z.sum(i, '*') <= M * x[i], name='supply{},{}'.format(i,j))

    obj1 = quicksum(c[i,j] * d[j] * z[i,j] for i in range(n) for j in range(m))
    obj2 = quicksum(f[i] * x[i] for i in range(n))
    obj = obj1 + obj2
    model.setObjective(obj, GRB.MINIMIZE)

    model.update()
    model.write("lp.lp")
    model.optimize()
    x_values = np.array([x[i].x for i in range(n)])
    z_values = np.array([[z[i,j].x for j in range(m)] for i in range(n)])
    obj_value = model.objVal

    print(x_values, z_values)
    print('optimal:', obj_value)
    return SimpleNamespace(x=x_values, z=z_values, obj=obj_value)

def createQPMatrix(instance):
    # min  1/2 x^T Q x + x^T p
    # s.t. A x =  b
    #      G x <= h
    n, m, c, d, f = instance.n, instance.m, instance.c, instance.d, instance.f
    variable_size = n + n * m
    Q = torch.eye(vairable_size) * 0.01
    print(Q.shape)
    p = torch.cat((torch.Tensor(f), torch.Tensor(c * d)).flatten())
    print(p.shape)
    A = []
    for j in range(m):
        A.append([0] * j + [1] * 
    return Q, p, A, b, G, h

def train(dataset, lr=0.1, trining_method='two-stage', device='cpu'):
    # train a single epoch
    net.train()
    loss_fn = torch.nn.MSELoss()
    train_losses = []
    for batch_idx, (features, labels) in enumerate(tqdm.tqdm(dataset)):
        features, labels = features.to(device), labels.to(device)
        outputs = net(features)
        loss = loss_fn(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_losses.append(loss.item())

    average_loss = np.mean(train_losses)
    sys.stdout.write(f' | Train Loss: {average_loss:.3f}\n')
    sys.stdout.flush()

def test(dataset, device='cpu'):
    # test a single epoch
    net.eval()
    loss_fn = torch.nn.MSELoss()
    test_losses = []
    for batch_idx, (features, labels) in enumerate(tqdm.tqdm(dataset)):
        features, labels = features.to(device), labels.to(device)
        outputs = net(features)
        loss = loss_fn(outputs, labels)
        test_losses.append(loss.item())
    average_loss = np.mean(test_losses)
    sys.stdout.write(f' | Test Loss: {average_loss:.3f}\n')
    sys.stdout.flush()

if __name__ == '__main__':
    n, m = 20, 100 # n: # of facilities, m: # of customers
    sample_instance = generateInstance(n, m)
    sample_instance.c = None
    # print("MILP solver")
    # MILPSolver(instance)

    # print("LP solver")
    # LPSolver(instance)

    num_instances = 16000
    feature_size = 32
    lr = 0.001
    dataset = generateDataset(n, m, num_instances, feature_size)

    net = FacilityNN(input_shape=(n,feature_size), output_shape=(n,m))
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)

    num_epochs = 100
    for epoch in range(num_epochs):
        train(dataset.train)
        # validate(dataset.validate)
        test(dataset.test)


